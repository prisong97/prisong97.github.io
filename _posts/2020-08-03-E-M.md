---
title: "E-M Algorithm"
mathjax: true
layout: post
---
Assuming that our observations arise from one or more unobserved classes, the Expectation-Maximisation (EM) algorithm iteratively finds the maximum likelihood estimates of these classes’ parameters.

As a motivating example, let us consider a random variable $$X$$ and a set of $$m$$ i.i.d observations $$x_1, x_2, \ldots, x_m$$. The probability of receiving observation $$x_i$$ is $$\mathbb{P}(x_i, \theta)$$, where $$\theta$$ refers to the parameter(s) governing the pdf of distribution $$X$$. Since the observations are i.i.d, we can write the likelihood function as follows:

$$\mathbb{P}(X, \theta) = \prod_{i=1}^m\mathbb{P}(x_i, \theta)$$. 

As it is easier to deal with a sum of terms instead of a product, we usually introduce the log function and consider the log-likehood function instead. The log-likelihood function is then given by:

$$\log\mathbb{P}(X, \theta) = \sum_{i=1}^m\log\mathbb{P}(x_i, \theta)$$.

So far, we have not considered the different unobserved classes these observations belong to. If we know that $$X$$ comprises of $$n$$ subpopulations, then each class $$\in \{z_1,…,z_n\}$$ and $$\sum_z P(z\in Z) = 1$$. Consequently, we can re-write the log-likelihood as

$$
\begin{align} 
\log\mathbb{P}(X, \theta) &= \sum_{i=1}^m\log\mathbb{P}(x_i, \theta) \\ &= \sum_{i=1}^m \log \sum_{j=1}^n \mathbb{P}(x_i, \theta, \z_j).     
\end{align}
$$
      
 





