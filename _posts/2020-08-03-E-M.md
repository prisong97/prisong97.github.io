---
title: "E-M Algorithm"
mathjax: true
layout: post
---
Assuming that our observations arise from one or more unobserved classes, the Expectation-Maximisation (EM) algorithm iteratively finds the maximum likelihood estimates of these classes’ parameters.

As a motivating example, let us consider a random variable $$X$$ and a set of $$m$$ i.i.d observations $$x^{(1)}, x^{(2)}, \ldots, x^{(m)}$$. The probability of receiving observation $$x^{(i)}$$ is $$P(x^{(i)}, \theta)$$, where $$\theta$$ refers to the parameter(s) governing the pdf of distribution $$X$$. Given that these observations are i.i.d, we can write the likelihood function as follows:

$$P(X, \theta) = \prod_{i=1}^m P(x^{(i)}, \theta)$$. 

Usually, it is easier to maximise the log-likelihood as opposed to the likelihood. To this end, we introduce the log function and consider the log-likehood function instead. The log-likelihood function is then given by:

$$\log P(X, \theta) = \sum_{i=1}^m\log P(x^{(i)}, \theta)$$.

So far, we have not considered the different unobserved classes these observations belong to. These unobserved classes are known as latent variables, which we deliberately introduce into the log-likelihood equation for purposes of carrying out the E-M algorithm. Let $Z$ denote the random variable representing these unobserved classes. In the case of a finite number of classes, if we know that $$X$$ comprises of $$n$$ subpopulations, then each class $$\in \{z_1,…,z_n\}$$. Additionally, let $Q$ denote the distribution over the $Z$. Then, $$\sum_z Q(z\in Z) = 1$$. Consequently, the log-likelihood can be written as

$$
\begin{align}
\log P(X, \theta) &= \sum_{i=1}^m\log P(x^{(i)}, \theta) \\
&= \sum_{i=1}^m \log \sum_{j=1}^n P(x^{(i)}, \theta, z^{(i)}_j) \\
&= \sum_{i=1}^m \log \sum_{j=1}^n Q(z^{(i)}) \frac{P(x^{(i)}_j, \theta, z^{(i)})}{Q(z^{(i)}_j)} \\
& \geq \sum_{i=1}^m  \sum_{j=1}^n Q(z^{(i)}_j) \log \frac{P(x^{(i)}, \theta, z^{(i)}_j)}{Q(z^{(i)}_j)}  \mbox{ (by Jensen's Inequality)}.
\end{align}
$$

For $(4)$ to hold with equality, it is sufficient to set $Q(z^{(i)}_j) = f(z^{(i)}_j \mid x^{(i)}, \theta)$ [a], the posterior probability of $z^{(i)}_j$ given that we have observed $x^{(i)}$.


### Bibliography ###
[a] http://cs229.stanford.edu/notes/cs229-notes8.pdf
 





